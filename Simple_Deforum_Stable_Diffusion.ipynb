{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "# **Mafia Diffusion v0.1**\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).  \n",
        "\n",
        "---\n",
        "\n",
        "Based on [deforum](https://discord.gg/upmXXsrwZc) notebook\n",
        "\n",
        "Tweaked by Prof. R.J#1965\n",
        "\n",
        "---\n",
        "\n",
        "**2022-09-02 update**  \n",
        "chloebubble#9999 [(Twitter)](https://twitter.com/0xCrung)\n",
        "  \n",
        "**changelog:**  \n",
        "  \n",
        "~ wrote/rewrote functions for file saving; prompts are now saved in filenames and optionally to a text file; things go neatly in their own directories now  \n",
        "~ various bugs and annoyances fixed  \n",
        "~ TODO: push updated qol functions; add a check to prevent repeated prompts being written to file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2g-f7cQmf2Nt"
      },
      "outputs": [],
      "source": [
        "#@markdown **NVIDIA GPU**\n",
        "import subprocess\n",
        "sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(sub_p_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4knibRpAQ06"
      },
      "source": [
        "# Setup and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VRNl2mfepEIe"
      },
      "outputs": [],
      "source": [
        "#@markdown **Setup Environment**\n",
        "\n",
        "setup_environment = True #@param {type:\"boolean\"}\n",
        "print_subprocess = True #@param {type:\"boolean\"}\n",
        "\n",
        "if setup_environment:\n",
        "    import subprocess\n",
        "    print(\"...setting up environment\")\n",
        "    all_process = [['pip', 'install', 'torch==1.11.0+cu113', 'torchvision==0.12.0+cu113', 'torchaudio==0.11.0', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'],\n",
        "                   ['pip', 'install', 'omegaconf==2.1.1', 'einops==0.3.0', 'pytorch-lightning==1.4.2', 'torchmetrics==0.6.0', 'torchtext==0.2.3', 'transformers==4.19.2', 'kornia==0.6'],\n",
        "                   ['git', 'clone', 'https://github.com/ProfRJ/stable-diffusion'],\n",
        "                   ['pip', 'install', '-e', 'git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers'],\n",
        "                   ['pip', 'install', '-e', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "                   ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'resize-right', 'torchdiffeq', 'python-slugify'],\n",
        "                 ]\n",
        "    for process in all_process:\n",
        "        running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if print_subprocess:\n",
        "            print(running)\n",
        "    \n",
        "    print(subprocess.run(['git', 'clone', 'https://github.com/deforum/k-diffusion/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    with open('k-diffusion/k_diffusion/__init__.py', 'w') as f:\n",
        "        f.write('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "81qmVZbrm4uu"
      },
      "outputs": [],
      "source": [
        "#@markdown **Python Definitions**\n",
        "import json\n",
        "from IPython import display\n",
        "from google.colab import output\n",
        "\n",
        "import argparse, glob, os, pathlib, subprocess, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import math\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from skimage.exposure import match_histograms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from types import SimpleNamespace\n",
        "from torch import autocast\n",
        "from os.path import exists\n",
        "from slugify import slugify, smart_truncate\n",
        "from slugify.__main__ import slugify_params, parse_args\n",
        "\n",
        "sys.path.append('./src/taming-transformers')\n",
        "sys.path.append('./src/clip')\n",
        "sys.path.append('./stable-diffusion/')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "from helpers import save_samples, sampler_fn\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion import sampling\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "from qol_functions import split_weighted_subprompts, setres, get_output_folder, load_img, next_seed, split_batches_from_samples, get_gpu_information\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model, args):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "        self.latent_truncation = args.latent_truncation\n",
        "        self.threshold = args.threshold\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "\n",
        "        res = uncond + (cond - uncond) * cond_scale\n",
        "        if self.latent_truncation: \n",
        "            maxval = 0.0 + torch.max(res).cpu().numpy()\n",
        "            minval = 0.0 + torch.min(res).cpu().numpy()\n",
        "            if maxval < self.threshold and minval > -self.threshold:\n",
        "                return res\n",
        "            if maxval > self.threshold:\n",
        "                maxval = min(max(1, 0.707*maxval), self.threshold)\n",
        "            if minval < -self.threshold:\n",
        "                minval = max(min(-1, 0.707*minval), -self.threshold)\n",
        "            return torch.clamp(res, min=minval, max=maxval)\n",
        "        else:\n",
        "            return res\n",
        "\n",
        "def make_callback(sampler, dynamic_threshold=None, static_threshold=None):  \n",
        "    # Creates the callback function to be passed into the samplers\n",
        "    # The callback function is applied to the image after each step\n",
        "    def dynamic_thresholding_(img, threshold):\n",
        "        # Dynamic thresholding from Imagen paper (May 2022)\n",
        "        s = np.percentile(np.abs(img.cpu()), threshold, axis=tuple(range(1,img.ndim)))\n",
        "        s = np.max(np.append(s,1.0))\n",
        "        torch.clamp_(img, -1*s, s)\n",
        "        torch.FloatTensor.div_(img, s)\n",
        "\n",
        "    # Callback for samplers in the k-diffusion repo, called thus:\n",
        "    #   callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "    def k_callback(args_dict):\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(args_dict['x'], -1*static_threshold, static_threshold)\n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(args_dict['x'], dynamic_threshold)\n",
        "\n",
        "    # Function that is called on the image (img) and step (i) at each step\n",
        "    def img_callback(img, i):\n",
        "        # Thresholding functions\n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(img, dynamic_threshold)\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(img, -1*static_threshold, static_threshold)\n",
        "\n",
        "    if sampler in [\"plms\",\"ddim\"]: \n",
        "        # Callback function formated for compvis latent diffusion samplers\n",
        "        callback = img_callback\n",
        "    else: \n",
        "        # Default callback function uses k-diffusion sampler variables\n",
        "        callback = k_callback\n",
        "\n",
        "    return callback\n",
        "\n",
        "def generate(args, return_latent=False, return_sample=False, return_c=False):\n",
        "    seed_everything(args.seed)\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    if args.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    results = []\n",
        "    for batch_sequence in range(args.batch_sequences):\n",
        "        batch_size = args.batch_size_schedule[batch_sequence]\n",
        "        args.n_samples = batch_size\n",
        "        init_latent = None\n",
        "        if args.init_latent is not None:\n",
        "            init_latent = args.init_latent\n",
        "        elif args.init_sample is not None:\n",
        "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(args.init_sample))\n",
        "        elif args.init_image != None and args.init_image != '':\n",
        "            init_image = load_img(args.init_image, shape=(args.W, args.H)).to(device)\n",
        "            init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space        \n",
        "\n",
        "        sampler.make_schedule(ddim_num_steps=args.steps, ddim_eta=args.ddim_eta, verbose=False)\n",
        "\n",
        "        t_enc = int((1.0-args.strength) * args.steps)\n",
        "\n",
        "        start_code = None\n",
        "        if args.fixed_code and init_latent == None:\n",
        "            start_code = torch.randn([batch_size, args.C, args.H // args.f, args.W // args.f], device=device)\n",
        "\n",
        "        callback = make_callback(sampler=args.sampler,\n",
        "                                dynamic_threshold=args.dynamic_threshold, \n",
        "                                static_threshold=args.static_threshold)\n",
        "\n",
        "        precision_scope = autocast if args.precision == \"autocast\" else nullcontext\n",
        "        with torch.no_grad():\n",
        "            with precision_scope(\"cuda\"):\n",
        "                with model.ema_scope():\n",
        "                    for n in range(1):\n",
        "                        model_wrap = CompVisDenoiser(model, args)       \n",
        "                        prompt = args.prompt\n",
        "                        data = [batch_size * [prompt]]\n",
        "                        assert prompt is not None\n",
        "                        for prompts in data:\n",
        "                            uc = None\n",
        "                            if args.scale != 1.0:\n",
        "                                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                            if isinstance(prompts, tuple):\n",
        "                                prompts = list(prompts)\n",
        "                            c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                            if args.init_c != None:\n",
        "                                c = args.init_c\n",
        "\n",
        "                            if args.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "                                shape = [args.C, args.H // args.f, args.W // args.f]\n",
        "                                sigmas = model_wrap.get_sigmas(args.steps)\n",
        "                                if args.use_init:\n",
        "                                    sigmas = sigmas[len(sigmas)-t_enc-1:]\n",
        "                                    x = init_latent + torch.randn([batch_size, *shape], device=device) * sigmas[0]\n",
        "                                else:\n",
        "                                    x = torch.randn([args.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap, args)\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': args.scale}\n",
        "                                if args.sampler==\"klms\":\n",
        "                                    samples = sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                                elif args.sampler==\"dpm2\":\n",
        "                                    samples = sampling.sample_dpm_2(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                                elif args.sampler==\"dpm2_ancestral\":\n",
        "                                    samples = sampling.sample_dpm_2_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                                elif args.sampler==\"heun\":\n",
        "                                    samples = sampling.sample_heun(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                                elif args.sampler==\"euler\":\n",
        "                                    samples = sampling.sample_euler(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                                elif args.sampler==\"euler_ancestral\":\n",
        "                                    samples = sampling.sample_euler_ancestral(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False, callback=callback)\n",
        "                            else:\n",
        "\n",
        "                                if init_latent != None:\n",
        "                                    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                                    samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=args.scale,\n",
        "                                                            unconditional_conditioning=uc,)\n",
        "                                else:\n",
        "                                    if args.sampler == 'plms' or args.sampler == 'ddim':\n",
        "                                        shape = [args.C, args.H // args.f, args.W // args.f]\n",
        "                                        samples, _ = sampler.sample(S=args.steps,\n",
        "                                                                        conditioning=c,\n",
        "                                                                        batch_size=args.n_samples,\n",
        "                                                                        shape=shape,\n",
        "                                                                        verbose=False,\n",
        "                                                                        unconditional_guidance_scale=args.scale,\n",
        "                                                                        unconditional_conditioning=uc,\n",
        "                                                                        eta=args.ddim_eta,\n",
        "                                                                        x_T=start_code,\n",
        "                                                                        img_callback=callback)\n",
        "\n",
        "                            if return_latent:\n",
        "                                results.append(samples.clone())\n",
        "\n",
        "                            x_samples = model.decode_first_stage(samples)\n",
        "                            if return_sample:\n",
        "                                results.append(x_samples.clone())\n",
        "\n",
        "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                            if return_c:\n",
        "                                results.append(c.clone())\n",
        "\n",
        "                            for x_sample in x_samples:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                image = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                                results.append(image)\n",
        "    return results\n",
        "\n",
        "def sample_from_cv2(sample: np.ndarray) -> torch.Tensor:\n",
        "    sample = ((sample.astype(float) / 255.0) * 2) - 1\n",
        "    sample = sample[None].transpose(0, 3, 1, 2).astype(np.float16)\n",
        "    sample = torch.from_numpy(sample)\n",
        "    return sample\n",
        "\n",
        "def sample_to_cv2(sample: torch.Tensor) -> np.ndarray:\n",
        "    sample_f32 = rearrange(sample.squeeze().cpu().numpy(), \"c h w -> h w c\").astype(np.float32)\n",
        "    sample_f32 = ((sample_f32 * 0.5) + 0.5).clip(0, 1)\n",
        "    sample_int8 = (sample_f32 * 255).astype(np.uint8)\n",
        "    return sample_int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TxIOPT0G5Lx1"
      },
      "outputs": [],
      "source": [
        "#@markdown **Model and Output Paths**\n",
        "# ask for the link\n",
        "print(\"Local Path Variables:\\n\")\n",
        "\n",
        "models_path = \"/content/models\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Google Drive Path Variables (Optional)**\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False\n",
        "\n",
        "if mount_google_drive:\n",
        "    from google.colab import drive # type: ignore\n",
        "    try:\n",
        "        drive_path = \"/content/drive\"\n",
        "        drive.mount(drive_path,force_remount=force_remount)\n",
        "        models_path_gdrive = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "        output_path_gdrive = \"/content/drive/MyDrive/DiffusionOutput/StableDiffusion\" #@param {type:\"string\"}\n",
        "        models_path = models_path_gdrive\n",
        "        output_path = output_path_gdrive\n",
        "    except:\n",
        "        print(\"...error mounting drive or with drive path variables\")\n",
        "        print(\"...reverting to default path variables\")\n",
        "\n",
        "import os\n",
        "os.makedirs(models_path, exist_ok=True)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CIUJ7lWI4v53"
      },
      "outputs": [],
      "source": [
        "#@markdown **Select and Load Model**\n",
        "\n",
        "model_config = \"v1-inference.yaml\" #@param [\"custom\",\"v1-inference.yaml\"]\n",
        "model_checkpoint =  \"sd-v1-4.ckpt\" #@param [\"custom\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\"]\n",
        "custom_config_path = \"\" #@param {type:\"string\"}\n",
        "custom_checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "check_sha256 = True #@param {type:\"boolean\"}\n",
        "\n",
        "load_on_run_all = True #@param {type: 'boolean'}\n",
        "half_precision = True # needs to be fixed\n",
        "\n",
        "model_map = {\n",
        "    \"sd-v1-4-full-ema.ckpt\": {'sha256': '14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a'},\n",
        "    \"sd-v1-4.ckpt\": {'sha256': 'fe4efff1e174c627256e44ec2991ba279b3816e364b49f9be2abc0b3ff3f8556'},\n",
        "    \"sd-v1-3-full-ema.ckpt\": {'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca'},\n",
        "    \"sd-v1-3.ckpt\": {'sha256': '2cff93af4dcc07c3e03110205988ff98481e86539c51a8098d4f2236e41f7f2f'},\n",
        "    \"sd-v1-2-full-ema.ckpt\": {'sha256': 'bc5086a904d7b9d13d2a7bccf38f089824755be7261c7399d92e555e1e9ac69a'},\n",
        "    \"sd-v1-2.ckpt\": {'sha256': '3b87d30facd5bafca1cbed71cfb86648aad75d1c264663c0cc78c7aea8daec0d'},\n",
        "    \"sd-v1-1-full-ema.ckpt\": {'sha256': 'efdeb5dc418a025d9a8cc0a8617e106c69044bc2925abecc8a254b2910d69829'},\n",
        "    \"sd-v1-1.ckpt\": {'sha256': '86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea'}\n",
        "}\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', url, '-v', '--show-progress', '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "\n",
        "# config path\n",
        "ckpt_config_path = custom_config_path if model_config == \"custom\" else os.path.join(models_path, model_config)\n",
        "if os.path.exists(ckpt_config_path):\n",
        "    print(f\"{ckpt_config_path} exists\")\n",
        "else:\n",
        "    ckpt_config_path = \"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
        "print(f\"Using config: {ckpt_config_path}\")\n",
        "\n",
        "# checkpoint path or download\n",
        "ckpt_path = custom_checkpoint_path if model_checkpoint == \"custom\" else os.path.join(models_path, model_checkpoint)\n",
        "\n",
        "\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"{ckpt_path} exists\")\n",
        "    ckpt_valid = True\n",
        "else:\n",
        "    try: \n",
        "        download_link = f'http://batbot.ai/models/stable-diffusion/{model_checkpoint}'\n",
        "        print(f\"!wget -O {models_path}/{model_checkpoint} {download_link}\")\n",
        "        wget(download_link, models_path)\n",
        "        ckpt_valid = True\n",
        "    except:\n",
        "        os.remove(os.path.join(models_path, model_checkpoint))\n",
        "        print(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n",
        "        ckpt_valid = False\n",
        "\n",
        "if check_sha256 and model_checkpoint != \"custom\" and ckpt_valid:\n",
        "    import hashlib\n",
        "    print(\"\\n...checking sha256\")\n",
        "    with open(ckpt_path, \"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest()\n",
        "        del bytes\n",
        "    if model_map[model_checkpoint][\"sha256\"] == hash:\n",
        "        print(\"hash is correct\\n\")\n",
        "    else:\n",
        "        print(\"hash in not correct\\n\")\n",
        "        ckpt_valid = False\n",
        "\n",
        "if ckpt_valid:\n",
        "    print(f\"Using ckpt: {ckpt_path}\")\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda', half_precision=True):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    if half_precision:\n",
        "        model = model.half().to(device)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "if load_on_run_all and ckpt_valid:\n",
        "    local_config = OmegaConf.load(f\"{ckpt_config_path}\")\n",
        "    model = load_model_from_config(local_config, f\"{ckpt_path}\",half_precision=half_precision)\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcxAWpLTK2AO"
      },
      "source": [
        "### Temporary place for qol module script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "985V4-K9NZaX"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from torchvision.datasets.utils import download_url\n",
        "from ldm.util import instantiate_from_config\n",
        "import torch\n",
        "import os\n",
        "# todo ?\n",
        "from google.colab import files\n",
        "from IPython.display import Image as ipyimg\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from einops import rearrange, repeat\n",
        "import torch, torchvision\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.util import ismap\n",
        "import time\n",
        "from omegaconf import OmegaConf\n",
        "import requests\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as TF\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "def add_noise(sample: torch.Tensor, noise_amt: float):\n",
        "    return sample + torch.randn(sample.shape, device=sample.device) * noise_amt\n",
        "\n",
        "def split_weighted_subprompts(text):\n",
        "    \"\"\"\n",
        "    grabs all text up to the first occurrence of ':' \n",
        "    uses the grabbed text as a sub-prompt, and takes the value following ':' as weight\n",
        "    if ':' has no value defined, defaults to 1.0\n",
        "    repeats until no text remaining\n",
        "    \"\"\"\n",
        "    remaining = len(text)\n",
        "    prompts = []\n",
        "    weights = []\n",
        "    while remaining > 0:\n",
        "        if \":\" in text:\n",
        "            idx = text.index(\":\") # first occurrence from start\n",
        "            # grab up to index as sub-prompt\n",
        "            prompt = text[:idx]\n",
        "            remaining -= idx\n",
        "            # remove from main text\n",
        "            text = text[idx+1:]\n",
        "            # find value for weight \n",
        "            if \" \" in text:\n",
        "                idx = text.index(\" \") # first occurence\n",
        "            else: # no space, read to end\n",
        "                idx = len(text)\n",
        "            if idx != 0:\n",
        "                try:\n",
        "                    weight = float(text[:idx])\n",
        "                except: # couldn't treat as float\n",
        "                    print(f\"Warning: '{text[:idx]}' is not a value, are you missing a space?\")\n",
        "                    weight = 1.0\n",
        "            else: # no value found\n",
        "                weight = 1.0\n",
        "            # remove from main text\n",
        "            remaining -= idx\n",
        "            text = text[idx+1:]\n",
        "            # append the sub-prompt and its weight\n",
        "            prompts.append(prompt)\n",
        "            weights.append(weight)\n",
        "        else: # no : found\n",
        "            if len(text) > 0: # there is still text though\n",
        "                # take remainder as weight 1\n",
        "                prompts.append(text)\n",
        "                weights.append(1.0)\n",
        "            remaining = 0\n",
        "    return prompts, weights\n",
        "\n",
        "def setres(image_shape, W, H):\n",
        "    image_shape, _, _ = image_shape.partition(' |')\n",
        "    return {\n",
        "        \"Custom\": (W, H),\n",
        "        \"Square\": (512, 512),\n",
        "        \"Large Square\": (768, 768),\n",
        "        \"Landscape\": (704, 512),\n",
        "        \"Large Landscape\": (767, 640),\n",
        "        \"Portrait\": (512, 704),\n",
        "        \"Large Portrait\": (640, 768)  \n",
        "    }.get(image_shape)\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "    yearMonth = time.strftime('%Y-%m')\n",
        "    out_path = os.path.join(output_path,yearMonth)\n",
        "    if batch_folder != \"\":\n",
        "        out_path = os.path.join(out_path,batch_folder)\n",
        "        # we will also make sure the path suffix is a slash if linux and a backslash if windows\n",
        "        #if out_path[-1] != os.path.sep:\n",
        "        #    out_path += os.path.sep\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    return out_path\n",
        "\n",
        "def get_prompts_folder(output_path,batch_folder=None,save_prompts_file=False):\n",
        "    yearMonth = time.strftime('%Y-%m/')\n",
        "    out_path = os.path.join(output_path,yearMonth)\n",
        "    if save_prompts_file:\n",
        "      if batch_folder != \"\":\n",
        "        prompts_folder = os.path.join(out_path,batch_folder,'prompts')\n",
        "      else:\n",
        "        prompts_folder = os.path.join(out_path,'prompts')\n",
        "      if out_path[-1] != os.path.sep:\n",
        "        out_path += os.path.sep\n",
        "    os.makedirs(prompts_folder, exist_ok=True)\n",
        "    return prompts_folder\n",
        "\n",
        "\n",
        "def load_img(path, shape):\n",
        "    \n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "    fac = max(shape[0] / image.size[0], shape[1] / image.size[1])\n",
        "    image = image.resize((int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    image = TF.center_crop(image, shape[::-1])\n",
        "    return 2.*image - 1.\n",
        "    \n",
        "\n",
        "def make_grid(images):\n",
        "    mode = images[0].mode\n",
        "    size = images[0].size\n",
        "\n",
        "    n = len(images)\n",
        "    x = math.ceil(n**0.5)\n",
        "    y = math.ceil(n / x)\n",
        "\n",
        "    output = Image.new(mode, (size[0] * x, size[1] * y))\n",
        "    for i, image in enumerate(images):\n",
        "        cur_x, cur_y = i % x, i // x\n",
        "        output.paste(image, (size[0] * cur_x, size[1] * cur_y))\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_gpu_information(image_size):\n",
        "    memory = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    memory = memory.split(', ')[1].strip(' MiB')\n",
        "    path = f'{os.getcwd()}/stable-diffusion/helpers/gpu-info'\n",
        "    f = open(f\"{path}/{memory}.txt\",\"r\")\n",
        "    lines = f.readlines()\n",
        "    max_samples = 0\n",
        "    for line in lines:\n",
        "        line = line.split(' | ')\n",
        "        max_res = int(line[1])\n",
        "        if max_res >= image_size:\n",
        "            max_samples = int(line[0])\n",
        "            continue\n",
        "        else:\n",
        "\n",
        "            break\n",
        "    if max_samples == 0:\n",
        "        raise error_message(\"Specified resolution is too large to fit on vram.\")\n",
        "    return max_samples\n",
        "\n",
        "\n",
        "def split_batches_from_samples(n_samples, image_size):\n",
        "    remaining_samples = n_samples\n",
        "    batch_size_schedule = []\n",
        "    max_samples = get_gpu_information(image_size)\n",
        "    batch_sequences = int(math.ceil(n_samples/max_samples))\n",
        "    for batch_sequence in range(batch_sequences):\n",
        "        while remaining_samples > 0:\n",
        "            if remaining_samples/max_samples <= 1:\n",
        "                batch_size_schedule.append(remaining_samples)\n",
        "                remaining_samples -= remaining_samples\n",
        "            else:\n",
        "                batch_size_schedule.append(max_samples)\n",
        "                remaining_samples -= max_samples\n",
        "    return (batch_sequences, batch_size_schedule)\n",
        "\n",
        "\n",
        "def next_seed(args):\n",
        "    if args.seed_behavior == 'iter':\n",
        "        args.seed += 1\n",
        "    elif args.seed_behavior == 'fixed':\n",
        "        pass # always keep seed the same\n",
        "    else:\n",
        "        args.seed = random.randint(0, 2**32)\n",
        "    return args.seed\n",
        "\n",
        "class error_message(Exception):\n",
        "       pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8RAo2zI-vQm"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ujwkGZTcGev"
      },
      "outputs": [],
      "source": [
        "full_prompts = [\n",
        "          # \"cyberpunk horror render of a masterfully sculpted, beautifully decaying porcelain cyborg princess, 4k cgsociety portrait render by WLOP\"\n",
        "          #'steampunk transcendence by Greg Rutkowski and Alex Grey, cyberpunk transcendence by Ross Tran and Wojciech Siudmak, conceptartworld 4k, outsider art inspired high detail path traced architectural render, colossal aztec-cyberpunk hybrid deities',\n",
        "          #'a magical portal to the vaporwave dimension, enchanted nostalgiacore 90s artwork by macintosh plus, greg rutkowski, steven belledin, 4k desktop wallpaper',\n",
        "          'cyberpunk horror render of a decaying mecha-ghoul by WLOP, cgsociety 4k portrait render, highly detailed filmic render',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "qH74gBWDd2oq"
      },
      "outputs": [],
      "source": [
        "def DeforumArgs():\n",
        "    #@markdown **Save & Display Settings**\n",
        "    batch_name = \"mechaghouls\" #@param {type:\"string\"}\n",
        "    outdir = get_output_folder(output_path, batch_name)\n",
        "    save_settings = True #@param {type:\"boolean\"}\n",
        "    save_samples = True #@param {type:\"boolean\"}\n",
        "    display_samples = True #@param {type:\"boolean\"}\n",
        "    save_prompts = True #@param {type:\"boolean\"}\n",
        "    prompts_folder = get_prompts_folder(output_path, batch_name, save_prompts)\n",
        "    #@markdown **Image Settings**\n",
        "    n_samples = 16 #@param {type:\"integer\"}\n",
        "    image_shape = \"Square | 512x512\" #@param [\"Custom\", \"Square | 512x512\", \"LargeSquare | 768x768\", \"Landscape | 704x512\", \"Large Landscape | 768x640\", \"Portrait | 512x704\", \"Large Portrait | 640x768\"]\n",
        "    W = 512 #@param {type:\"slider\", min:256, max:1536, step:64}\n",
        "    H = 512 #@param {type:\"slider\", min:256, max:1536, step:64}\n",
        "    (W, H) = setres(image_shape, W, H)\n",
        "\n",
        "    #@markdown **Init Settings**\n",
        "    use_init = False #@param {type:\"boolean\"}\n",
        "    strength = 0.25 #@param {type:\"number\"}\n",
        "    init_image = \"\" #@param {type:\"string\"}\n",
        "\n",
        "    #@markdown **Sampling Settings**\n",
        "    seed = -1 #@param\n",
        "    sampler = 'euler_ancestral' #@param [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"]\n",
        "    steps = 25 #@param\n",
        "    scale = 7 #@param\n",
        "    ddim_eta = 0.0 #@param\n",
        "    latent_truncation = True\n",
        "    threshold = 2.5\n",
        "\n",
        "    dynamic_threshold = None\n",
        "    static_threshold = None   \n",
        "\n",
        "    #@markdown **Batch Settings**\n",
        "    n_batch = 1 #@param\n",
        "    seed_behavior = \"random\" #@param [\"iter\",\"fixed\",\"random\"]\n",
        "\n",
        "    #@markdown **Grid Settings**\n",
        "    make_grid = True #@param {type:\"boolean\"}\n",
        "\n",
        "    (batch_sequences, batch_size_schedule) = split_batches_from_samples(n_samples, W*H)\n",
        "    \n",
        "    precision = 'autocast' \n",
        "    fixed_code = True\n",
        "    C = 4\n",
        "    f = 8\n",
        "\n",
        "    prompt = \"\"\n",
        "    timestring = \"\"\n",
        "    init_latent = None\n",
        "    init_sample = None\n",
        "    init_c = None\n",
        "\n",
        "    return locals()\n",
        "\n",
        "\n",
        "args = SimpleNamespace(**DeforumArgs())\n",
        "args.timestring = time.strftime('%Y-%m-%d-%H%M')\n",
        "args.strength = max(0.0, min(1.0, args.strength))\n",
        "\n",
        "\n",
        "if args.seed == -1:\n",
        "    args.seed = random.randint(0, 2**32)\n",
        "if not args.use_init:\n",
        "    args.init_image = None\n",
        "    args.strength = 0\n",
        "if args.sampler == 'plms' and args.use_init:\n",
        "    print(f\"Init images aren't supported with PLMS yet, switching to KLMS\")\n",
        "    args.sampler = 'klms'\n",
        "if args.sampler != 'ddim':\n",
        "    args.ddim_eta = 0\n",
        "\n",
        "# save full unsanitised prompts in a text file\n",
        "if args.save_prompts:\n",
        "  os.makedirs(args.prompts_folder, exist_ok=True)\n",
        "  os.path.join(args.prompts_folder)\n",
        "  prompts_list = '\\n'.join(map(str,full_prompts))\n",
        "  prompts_path = f'{args.prompts_folder}/prompts.txt'\n",
        "  if not os.path.exists(prompts_path):\n",
        "    open(prompts_path, 'w').close()\n",
        "  with open(prompts_path, 'a+') as file:\n",
        "    file.write(prompts_list + \"\\n\")\n",
        "  \n",
        "\n",
        "def render_image_batch(args):\n",
        "    # create output folder for the batch\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    if args.save_settings or args.save_samples:\n",
        "        print(f\"Saving to {os.path.join(args.outdir, args.timestring)}_*\")            \n",
        "\n",
        "    # save settings for the batch\n",
        "    if args.save_settings:\n",
        "      outdir_settings = f'{args.outdir}/settings'\n",
        "      os.makedirs(outdir_settings, exist_ok=True)\n",
        "      settings_dict = dict(args.__dict__)\n",
        "      settings_filename = os.path.join(outdir_settings, f\"{args.timestring}_settings.txt\")\n",
        "    \n",
        "    index = 0\n",
        "    \n",
        "    # function for init image batching\n",
        "    init_array = []\n",
        "    if args.use_init:\n",
        "        if args.init_image == \"\":\n",
        "            raise FileNotFoundError(\"No path was given for init_image\")\n",
        "        if args.init_image.startswith('http://') or args.init_image.startswith('https://'):\n",
        "            init_array.append(args.init_image)\n",
        "        elif not os.path.isfile(args.init_image):\n",
        "            if args.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n",
        "                args.init_image += \"/\" \n",
        "            for image in sorted(os.listdir(args.init_image)): # iterates dir and appends images to init_array\n",
        "                if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n",
        "                    init_array.append(args.init_image + image)\n",
        "        else:\n",
        "            init_array.append(args.init_image)\n",
        "    else:\n",
        "        init_array = [\"\"]\n",
        "\n",
        "    # when doing large batches don't flood browser with images\n",
        "    clear_between_batches = args.n_batch >= 32 or args.n_samples*args.n_batch >= 32 \n",
        "    \n",
        "    print('n_samples will run over', args.batch_sequences, 'batches with the following schedule', args.batch_size_schedule)\n",
        "    for iprompt, prompt in enumerate(full_prompts):  \n",
        "        \n",
        "        all_images = []\n",
        "\n",
        "        for batch_index in range(args.n_batch):\n",
        "            if clear_between_batches: \n",
        "                display.clear_output(wait=True)            \n",
        "            print(f\"Batch {batch_index+1} of {args.n_batch}\")\n",
        "            \n",
        "            for image in init_array: # iterates the init images\n",
        "                args.init_image = image\n",
        "                text = split_weighted_subprompts(prompt)\n",
        "                args.prompt, args.weight = text[0][0], text[1][0]\n",
        "                results = generate(args)\n",
        "                for image in results:\n",
        "                    if args.make_grid:\n",
        "                        all_images.append(T.functional.pil_to_tensor(image))\n",
        "                    if args.save_samples:\n",
        "                        sanitised_prompt = slugify(args.prompt, replacements=[[',', '_'],['.','.']], separator='_', max_length=96)\n",
        "                        filename = f'{args.timestring}_{index:05}_{sanitised_prompt}_{args.seed}.png'\n",
        "                        image.save(os.path.join(args.outdir, filename))\n",
        "                    if args.display_samples:\n",
        "                        display.display(image)\n",
        "                    index += 1\n",
        "                args.seed = next_seed(args)\n",
        "\n",
        "        # Make grid of entire prompt run\n",
        "        if args.make_grid:\n",
        "            outdir_grid = f'{args.outdir}/grid'\n",
        "            os.makedirs(outdir_grid, exist_ok=True)\n",
        "            grid = torchvision.utils.make_grid(all_images, nrow=int(math.ceil(len(all_images)**0.5)), padding=0)\n",
        "            grid = rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "            sanitised_prompt = slugify(args.prompt, replacements=[[',', '_'],['.','.']], separator='_', max_length=96)\n",
        "            filename = f'{args.timestring}_grid_{sanitised_prompt}_{args.seed}.png'\n",
        "            grid_image = Image.fromarray(grid.astype(np.uint8))\n",
        "            grid_image.save(os.path.join(outdir_grid, filename))\n",
        "            #display.clear_output(wait=True)            \n",
        "            display.display(grid_image)\n",
        "\n",
        "    # save settings for the batch\n",
        "    if args.save_settings:\n",
        "        with open(settings_filename, \"w+\", encoding=\"utf-8\") as f:\n",
        "            json.dump(dict(settings_dict), f, ensure_ascii=False, indent=4)\n",
        "        print(f'Settings saved to {settings_filename}')\n",
        "    if args.save_prompts:\n",
        "        print(f'Prompts saved to {prompts_path}')\n",
        "render_image_batch(args)    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMJBzX8UCnkTtbwURKHwZ4y",
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "87eb9abeca320fd1cef5175c021f0de4b3b214876fafb118601010f9be1316a9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
